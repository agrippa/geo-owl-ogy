{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BP Smoky Mountain Data Challenge\n",
    "\n",
    "#### Author: Max Grossman (max.grossman@bp.com)\n",
    "\n",
    "This notebook is intended to be an illustrative example of loading the realizations and gathers provided as part of the BP problem in the 2020 Smoky Mountain Data Challenge. This notebook relies on data files kept under the small_dataset/ folder provided with the data challenge. Please ensure that the folder is downloaded and unzipped, and placed in the same directory as this notebook.\n",
    "\n",
    "This notebook requires that the numpy, matplotlib, pillow, and segpy packages be installed in the current Python environment. numpy, matplotlib, and pillow may be installed using pip or conda. segpy is only available from pip (i.e. 'pip install segpy')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import segpy\n",
    "from segpy.reader import create_reader\n",
    "from segpy.writer import write_segy\n",
    "\n",
    "import math\n",
    "import matplotlib as plot\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from skimage import data, img_as_float\n",
    "from skimage.measure import compare_ssim as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility for pulling information out of SEGY header and printing it for the user.\n",
    "\"\"\"\n",
    "def print_segy_info(filename):\n",
    "    with open(filename, 'rb') as segy_in_file:\n",
    "        segy_reader = create_reader(segy_in_file, endian='>')\n",
    "\n",
    "        print(\"Filename:             \", segy_reader.filename)\n",
    "        print(\"SEG Y revision:       \", segy_reader.revision)\n",
    "        print(\"Number of traces:     \", segy_reader.num_traces())\n",
    "        print(\"Data format:          \",\n",
    "              segy_reader.data_sample_format_description)\n",
    "        print(\"Dimensionality:       \", segy_reader.dimensionality)\n",
    "\n",
    "        print()\n",
    "        print(\"=== BEGIN TEXTUAL REEL HEADER ===\")\n",
    "        for line in segy_reader.textual_reel_header:\n",
    "            if len(line.strip()) > 0:\n",
    "                print(line)\n",
    "        print(\"=== END TEXTUAL REEL HEADER ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The provided files ending with a .stack.segy suffix store realizations of the subsurface\n",
    "using some plausible (but not perfect) velocity model.\n",
    "\n",
    "These realizations are 2D, with a depth (z) dimension of 400 and a horizontal (x) dimension\n",
    "of 1058. This information can be seen in the SEGY header displayed below.\n",
    "\"\"\"\n",
    "print_segy_info(\"C:/Users/19174/Desktop/BP HPC/large_dataset_only_realizations/img_1.stack.segy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of loading a single 2D realization from a single file into a 2D numpy array, with x\n",
    "as your leading dimension and z as your innermost dimension. We know the x and z dimensions\n",
    "(1058 and 400) from the SEGY header printed by the cell above.\n",
    "\"\"\"\n",
    "REALIZATIONS = 1000\n",
    "deviations = np.zeros((1058, 400))\n",
    "means = np.zeros((1058, 400))\n",
    "totals = np.zeros((1058, 400))\n",
    "PATH = \"C:/Users/19174/Desktop/BP HPC/large_dataset_only_realizations\"\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random_std = []\n",
    "for file in os.listdir(PATH):\n",
    "    if file.endswith(\".stack.segy\"):\n",
    "        print(file)\n",
    "        path = \"C:/Users/19174/Desktop/BP HPC/large_dataset_only_realizations/\"+ file\n",
    "        with open(path, 'rb') as segy_in_file:\n",
    "            segy_reader = create_reader(segy_in_file, endian='>')\n",
    "            for trace_index in segy_reader.trace_indexes():\n",
    "                data = segy_reader.trace_samples(trace_index)\n",
    "                for i in range(len(data)):\n",
    "                    means[trace_index, i] += data[i]/REALIZATIONS #Collecting means, storing in array\n",
    "                    temp = random.randint(0, 423200)\n",
    "                    if temp <= 100:\n",
    "                        x = float(data[i])\n",
    "                        random_std.append(x)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.Series(random_std)\n",
    "df.plot(kind='hist', figsize = (25, 6), bins=200, title = \"Distribution of Random Pixel Values across Realizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probably better way than to rerun over files :( \n",
    "for file in os.listdir(PATH):\n",
    "    if file.endswith(\".stack.segy\"):\n",
    "        print(file)\n",
    "        path = \"C:/Users/19174/Desktop/BP HPC/large_dataset_only_realizations/\"+ file\n",
    "        with open(path, 'rb') as segy_in_file:\n",
    "            segy_reader = create_reader(segy_in_file, endian='>')\n",
    "            for trace_index in segy_reader.trace_indexes():\n",
    "                data = segy_reader.trace_samples(trace_index)\n",
    "                for i in range(len(data)):\n",
    "                    totals[trace_index, i] += (data[i] - means[trace_index, i]) ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(\"Deviations: Rows\", len(deviations), \"Cols \", len(deviations[0]))\n",
    "#print(\"Cell - Mean: Rows\", len(totals) , \"Cols \", len(totals[0]))\n",
    "#print(\"Mean: Rows\", len(means), \"Cols\", len(means[0]))\n",
    "\n",
    "for i in range(len(deviations)):\n",
    "    for j in range(len(deviations[0])):\n",
    "\n",
    "        deviations[i][j] = math.sqrt(totals[i][j]/REALIZATIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviations.min(), deviations.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run once for large data\n",
    "with open('large_deviation.pickle', 'wb') as f:\n",
    "    pickle.dump(deviations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this to access large st deviation matrix\n",
    "with open('large_deviation.pickle', 'rb') as f:\n",
    "     deviations= pickle.load(f) #deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make copies of deviations to avoid manipulating true data\n",
    "flat_deviations = deviations.flatten()\n",
    "print(\"Max STD: \",deviations.max())\n",
    "df = pd.Series(flat_deviations)\n",
    "df.plot(kind='hist', figsize = (25, 6), bins=200, title = \"Distribution of Standard Deviations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_REAL = \"C:/Users/19174/Desktop/BP HPC/large_dataset_only_realizations/img_5.stack.segy\"\n",
    "sample_arr = np.zeros([1058, 400, 3], dtype=np.uint8)\n",
    "gray_sample_arr = np.zeros((1058, 400))\n",
    "with open(SAMPLE_REAL, 'rb') as segy_in_file:\n",
    "    segy_reader = create_reader(segy_in_file, endian='>')\n",
    "    for trace_index in segy_reader.trace_indexes():\n",
    "        data = segy_reader.trace_samples(trace_index)\n",
    "        for i in range(len(data)):\n",
    "            gray_sample_arr[trace_index, i] = data[i]\n",
    "with open(SAMPLE_REAL, 'rb') as segy_in_file:\n",
    "    segy_reader = create_reader(segy_in_file, endian='>')\n",
    "    for trace_index in segy_reader.trace_indexes():\n",
    "        data = segy_reader.trace_samples(trace_index)\n",
    "        for i in range(len(data)):\n",
    "            sample_arr[trace_index, i] = [data[i], 0, 0] #Collecting data for sample_arr\n",
    "\n",
    "min_arr = gray_sample_arr.min()\n",
    "max_arr = gray_sample_arr.max()\n",
    "gray_sample_arr = (gray_sample_arr - min_arr) / (max_arr - min_arr) #Normalizes all values to 0 - 1\n",
    "gray_sample_arr = gray_sample_arr * 255\n",
    "for i in range(len(gray_sample_arr)):\n",
    "    for j in range(len(gray_sample_arr[0])):\n",
    "        sample_arr[i][j] = [gray_sample_arr[i][j], gray_sample_arr[i][j], gray_sample_arr[i][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 2.0 #If STD > THRESHOLD plot\n",
    "deviations_copy = deviations.copy()\n",
    "max_dev = deviations_copy.max()\n",
    "for i in range(len(deviations_copy)):\n",
    "    for j in range(len(deviations_copy[0])):\n",
    "        if deviations_copy[i][j] >= THRESHOLD:\n",
    "            color_intensity = sample_arr[i][j][0]\n",
    "            sample_arr[i][j] = [color_intensity, 0, 0]\n",
    "\n",
    "im = Image.fromarray(sample_arr)\n",
    "im = im.convert('RGB')\n",
    "transposed  = im.transpose(Image.ROTATE_270)\n",
    "\n",
    "\n",
    "transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_sample_arr = gray_sample_arr.transpose() #Reformat \n",
    "min_arr = gray_sample_arr.min()\n",
    "max_arr = gray_sample_arr.max()\n",
    "gray_sample_arr = (gray_sample_arr - min_arr) / (max_arr - min_arr) #Normalizes all values to 0 - 1\n",
    "gray_sample_arr = gray_sample_arr * 255  #Multiply everything to RGB Scale\n",
    "im = Image.fromarray(gray_sample_arr)\n",
    "im = im.convert('RGB')\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_sample_arr = np.zeros((1058, 400))\n",
    "with open(\"C:/Users/19174/Desktop/BP HPC/small_dataset/small_dataset/img_1.stack.segy\", 'rb') as segy_in_file:\n",
    "    segy_reader = create_reader(segy_in_file, endian='>')\n",
    "    for trace_index in segy_reader.trace_indexes():\n",
    "        data = segy_reader.trace_samples(trace_index)\n",
    "        for i in range(len(data)):\n",
    "            gray_sample_arr[trace_index, i] = data[i]\n",
    "gray_sample_arr = gray_sample_arr.transpose() #Reformat \n",
    "min_arr = gray_sample_arr.min()\n",
    "max_arr = gray_sample_arr.max()\n",
    "gray_sample_arr = (gray_sample_arr - min_arr) / (max_arr - min_arr) #Normalizes all values to 0 - 1\n",
    "gray_sample_arr = gray_sample_arr * 255  #Multiply everything to RGB Scale\n",
    "im = Image.fromarray(gray_sample_arr)\n",
    "im = im.convert('RGB')\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_sample_arr = np.zeros((1058, 400))\n",
    "with open(\"C:/Users/19174/Desktop/BP HPC/small_dataset/small_dataset/img_2.stack.segy\", 'rb') as segy_in_file:\n",
    "    segy_reader = create_reader(segy_in_file, endian='>')\n",
    "    for trace_index in segy_reader.trace_indexes():\n",
    "        data = segy_reader.trace_samples(trace_index)\n",
    "        for i in range(len(data)):\n",
    "            gray_sample_arr[trace_index, i] = data[i]\n",
    "gray_sample_arr = gray_sample_arr.transpose() #Reformat \n",
    "min_arr = gray_sample_arr.min()\n",
    "max_arr = gray_sample_arr.max()\n",
    "gray_sample_arr = (gray_sample_arr - min_arr) / (max_arr - min_arr) #Normalizes all values to 0 - 1\n",
    "gray_sample_arr = gray_sample_arr * 255  #Multiply everything to RGB Scale\n",
    "im = Image.fromarray(gray_sample_arr)\n",
    "im = im.convert('RGB')\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Template of breaking up a single realization into 23X20 tiles\n",
    "\n",
    "first_real = \"C:/Users/19174/Desktop/BP HPC/small_dataset/small_dataset/img_1.stack.segy\"\n",
    "hori_size = 46\n",
    "vert_size = 20\n",
    "ans = []\n",
    "for i in range(int(1058/ hori_size)):\n",
    "    test = []\n",
    "    for j in range(int(400 / vert_size)):\n",
    "        test.append([])\n",
    "    ans.append(test)\n",
    "REALS = 39\n",
    "for _ in range(1, REALS + 1):\n",
    "    first_real = \"C:/Users/19174/Desktop/BP HPC/small_dataset/small_dataset/img_\" + str(_) +  \".stack.segy\"\n",
    "    for temp in range(_ + 1, REALS + 1):\n",
    "        second_real = \"C:/Users/19174/Desktop/BP HPC/small_dataset/small_dataset/img_\" + str(temp) + \".stack.segy\"\n",
    "        print(\"First File: \",first_real)\n",
    "        print(\"Second File: \", second_real)\n",
    "        x_index = 0\n",
    "        y_index = 0\n",
    "        with open(first_real, 'rb') as first_file, open(second_real, 'rb') as second_file:\n",
    "            first_reader = create_reader(first_file, endian = '>')\n",
    "            second_reader = create_reader(second_file, endian = '>')\n",
    "            #i and j are top left of partition array\n",
    "            for i in range(0, 1058, hori_size):\n",
    "                for j in range(0, 400, vert_size):\n",
    "                    first_map = np.zeros((hori_size,vert_size))\n",
    "                    second_map = np.zeros((hori_size,vert_size))\n",
    "                    #k and m responsible for partition array\n",
    "                    for k in range(i, i + hori_size):\n",
    "                        data1 = first_reader.trace_samples(k)\n",
    "                        data2 = second_reader.trace_samples(k)\n",
    "                        for m in range(j, j + vert_size): #Loop sets matrix to these elements\n",
    "                            x = k% hori_size\n",
    "                            y = m % vert_size\n",
    "                            first_map[x, y] = data1[m]\n",
    "                            second_map[x, y] = data2[m]\n",
    "                    if first_map.max() != 0 and first_map.min() != 0:\n",
    "                        first_map = first_map.transpose() #Reformat \n",
    "                        min_arr = first_map.min()\n",
    "                        max_arr = first_map.max()\n",
    "                        first_map = (first_map - min_arr) / (max_arr - min_arr) #Normalizes all values to 0 - 1\n",
    "                        first_map = first_map * 255  #Multiply everything to RGB Scale\n",
    "                        im1 = Image.fromarray(first_map)\n",
    "                        im1 = im1.convert('RGB')\n",
    "                    else:\n",
    "                        #white image\n",
    "                        im1 = Image.fromarray(first_map)\n",
    "                        im1 = im1.convert('RGB')\n",
    "                    if second_map.max() != 0 and second_map.min() != 0:\n",
    "                        second_map = second_map.transpose() #Reformat \n",
    "                        min_arr = second_map.min()\n",
    "                        max_arr = second_map.max()\n",
    "                        second_map = (second_map - min_arr) / (max_arr - min_arr) #Normalizes all values to 0 - 1\n",
    "                        second_map = second_map * 255  #Multiply everything to RGB Scale\n",
    "                        im2 = Image.fromarray(second_map)\n",
    "                        im2 = im2.convert('RGB')\n",
    "                    else:\n",
    "                        im2 = Image.fromarray(second_map)\n",
    "                        im2 = im2.convert('RGB')\n",
    "                    im1 = im1.save(\"image_1.jpg\")\n",
    "                    im2 = im2.save(\"image_2.jpg\")\n",
    "                    img1 = cv2.imread('image_1.jpg', cv2.IMREAD_UNCHANGED)\n",
    "                    img2 = cv2.imread('image_2.jpg', cv2.IMREAD_UNCHANGED)\n",
    "                    #All ssim seem to equal 1.0 or very close\n",
    "                    s = ssim(img1, img2, multichannel= True)\n",
    "                    ans[x_index][y_index].append(s)\n",
    "                    y_index += 1\n",
    "                x_index += 1\n",
    "                y_index = 0\n",
    "with open('ssim_map.pickle', 'wb') as f:\n",
    "    pickle.dump(ans, f)\n",
    "for i in range(len(ans)):\n",
    "    for j in range(len(ans[0])):\n",
    "        ans[i][j] = [np.mean(ans[i][j])]\n",
    "print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Render the loaded realization by transposing it (so that the image is oriented intuitively) and normalizing.\n",
    "\"\"\"\n",
    "deviations_copy = deviations.copy()\n",
    "deviations_copy = deviations_copy.transpose()\n",
    "min_arr = deviations_copy.min()\n",
    "max_arr = deviations_copy.max()\n",
    "deviations_copy = (deviations_copy - min_arr) / (max_arr - min_arr)\n",
    "deviations_copy = deviations_copy * 255\n",
    "\n",
    "im = Image.fromarray(deviations_copy)\n",
    "im = im.convert('RGB')\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The provided files ending with a .gather.segy suffix store offset pair gathers generated\n",
    "using some plausible (but not perfect) velocity model. Each offset pair gather file corresponds\n",
    "with a single realization.\n",
    "\n",
    "These gather files are 3D, with 39 offset pairs in the survey. The z and x dimensions are the\n",
    "same as the realizations, with an added dimension indicating the offset pair. This information\n",
    "can be seen in the SEGY header displayed below.\n",
    "\"\"\"\n",
    "print_segy_info('Desktop/BP HPC/small_dataset/small_dataset/img_3.gather.segy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load gathers into a 3D numpy array with dimensions (x, offsets, z).\n",
    "\"\"\"\n",
    "arr = np.zeros((1058, 39, 400))\n",
    "with open('Desktop/BP HPC/small_dataset/small_dataset/img_3.gather.segy', 'rb') as segy_in_file:\n",
    "    segy_reader = create_reader(segy_in_file, endian='>')\n",
    "    \n",
    "    count = 0\n",
    "    for trace_index in segy_reader.trace_indexes():\n",
    "        data = segy_reader.trace_samples(trace_index)\n",
    "        for i in range(len(data)):\n",
    "            arr[int(trace_index / 39), trace_index % 39, i] = data[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.min(), arr.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Render the loaded gathers.\n",
    "\"\"\"\n",
    "\n",
    "nrows = 1\n",
    "ncols = 10\n",
    "f, axarr = matplotlib.pyplot.subplots(nrows, ncols, figsize = (30, 25))\n",
    "\n",
    "for c in range(ncols):\n",
    "    # Pull out all offset pairs for a given x\n",
    "    sub_arr = arr[c, :, :]\n",
    "    sub_arr = sub_arr.transpose()\n",
    "    min_arr = sub_arr.min()\n",
    "    max_arr = sub_arr.max()\n",
    "    sub_arr = (sub_arr - min_arr) / (max_arr - min_arr)\n",
    "    sub_arr = sub_arr * 255\n",
    "    \n",
    "    axarr[c].imshow(sub_arr, cmap = 'gray')\n",
    "    \n",
    "    title = str(c)\n",
    "    axarr[c].set_title(title.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
