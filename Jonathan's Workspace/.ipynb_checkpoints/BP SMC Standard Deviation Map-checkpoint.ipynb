{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BP Smoky Mountain Data Challenge\n",
    "\n",
    "#### Author: Jonathan Sheng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segpy\n",
    "from segpy.reader import create_reader\n",
    "from segpy.writer import write_segy\n",
    "\n",
    "import math\n",
    "import matplotlib as plot\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import statistics\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from skimage import data, img_as_float\n",
    "from skimage.measure import compare_ssim as ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Deviation Code\n",
    "Run across all realizations to find standard deviation for each pixel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this to access large st deviation matrix\n",
    "with open('large_deviation.pickle', 'rb') as f:\n",
    "     deviations= pickle.load(f) #deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of loading a single 2D realization from a single file into a 2D numpy array, with x\n",
    "as your leading dimension and z as your innermost dimension. We know the x and z dimensions\n",
    "(1058 and 400) from the SEGY header printed by the cell above.\n",
    "\"\"\"\n",
    "REALIZATIONS = 1000\n",
    "deviations = np.zeros((1058, 400))\n",
    "means = np.zeros((1058, 400))\n",
    "totals = np.zeros((1058, 400))\n",
    "PATH = \"C:/Users/19174/Desktop/BP HPC/large_dataset_only_realizations\"\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_std = []\n",
    "for file in os.listdir(PATH):\n",
    "    if file.endswith(\".stack.segy\"):\n",
    "        print(file)\n",
    "        path = \"C:/Users/19174/Desktop/BP HPC/large_dataset_only_realizations/\"+ file\n",
    "        with open(path, 'rb') as segy_in_file:\n",
    "            segy_reader = create_reader(segy_in_file, endian='>')\n",
    "            for trace_index in segy_reader.trace_indexes():\n",
    "                data = segy_reader.trace_samples(trace_index)\n",
    "                for i in range(len(data)):\n",
    "                    means[trace_index, i] += data[i]/REALIZATIONS #Collecting means, storing in array\n",
    "                    temp = random.randint(0, 423200)\n",
    "                    if temp <= 100:\n",
    "                        x = float(data[i])\n",
    "                        random_std.append(x)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.Series(random_std)\n",
    "df.plot(kind='hist', figsize = (25, 6), bins=200, title = \"Distribution of Random Pixel Values across Realizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probably better way than to rerun over files :( \n",
    "for file in os.listdir(PATH):\n",
    "    if file.endswith(\".stack.segy\"):\n",
    "        print(file)\n",
    "        path = \"C:/Users/19174/Desktop/BP HPC/large_dataset_only_realizations/\"+ file\n",
    "        with open(path, 'rb') as segy_in_file:\n",
    "            segy_reader = create_reader(segy_in_file, endian='>')\n",
    "            for trace_index in segy_reader.trace_indexes():\n",
    "                data = segy_reader.trace_samples(trace_index)\n",
    "                for i in range(len(data)):\n",
    "                    totals[trace_index, i] += (data[i] - means[trace_index, i]) ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(\"Deviations: Rows\", len(deviations), \"Cols \", len(deviations[0]))\n",
    "#print(\"Cell - Mean: Rows\", len(totals) , \"Cols \", len(totals[0]))\n",
    "#print(\"Mean: Rows\", len(means), \"Cols\", len(means[0]))\n",
    "\n",
    "for i in range(len(deviations)):\n",
    "    for j in range(len(deviations[0])):\n",
    "        deviations[i][j] = math.sqrt(totals[i][j]/REALIZATIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviations.min(), deviations.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run once for large data\n",
    "with open('large_deviation.pickle', 'wb') as f:\n",
    "    pickle.dump(deviations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make copies of deviations to avoid manipulating true data\n",
    "flat_deviations = deviations.flatten()\n",
    "print(\"Max STD: \",deviations.max())\n",
    "df = pd.Series(flat_deviations)\n",
    "df.plot(kind='hist', figsize = (25, 6), bins=200, title = \"Distribution of Standard Deviations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_REAL = \"C:/Users/19174/Desktop/BP HPC/large_dataset_only_realizations/img_5.stack.segy\"\n",
    "sample_arr = np.zeros([1058, 400, 3], dtype=np.uint8)\n",
    "gray_sample_arr = np.zeros((1058, 400))\n",
    "with open(SAMPLE_REAL, 'rb') as segy_in_file:\n",
    "    segy_reader = create_reader(segy_in_file, endian='>')\n",
    "    for trace_index in segy_reader.trace_indexes():\n",
    "        data = segy_reader.trace_samples(trace_index)\n",
    "        for i in range(len(data)):\n",
    "            gray_sample_arr[trace_index, i] = data[i]\n",
    "with open(SAMPLE_REAL, 'rb') as segy_in_file:\n",
    "    segy_reader = create_reader(segy_in_file, endian='>')\n",
    "    for trace_index in segy_reader.trace_indexes():\n",
    "        data = segy_reader.trace_samples(trace_index)\n",
    "        for i in range(len(data)):\n",
    "            sample_arr[trace_index, i] = [data[i], 0, 0] #Collecting data for sample_arr\n",
    "\n",
    "min_arr = gray_sample_arr.min()\n",
    "max_arr = gray_sample_arr.max()\n",
    "gray_sample_arr = (gray_sample_arr - min_arr) / (max_arr - min_arr) #Normalizes all values to 0 - 1\n",
    "gray_sample_arr = gray_sample_arr * 255\n",
    "for i in range(len(gray_sample_arr)):\n",
    "    for j in range(len(gray_sample_arr[0])):\n",
    "        sample_arr[i][j] = [gray_sample_arr[i][j], gray_sample_arr[i][j], gray_sample_arr[i][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 2.0 #If STD > THRESHOLD plot\n",
    "deviations_copy = deviations.copy()\n",
    "for i in range(len(deviations_copy)):\n",
    "    for j in range(len(deviations_copy[0])):\n",
    "        if deviations_copy[i][j] >= THRESHOLD:\n",
    "            color_intensity = sample_arr[i][j][0]\n",
    "            sample_arr[i][j] = [color_intensity, 0, 0]\n",
    "\n",
    "im = Image.fromarray(sample_arr)\n",
    "im = im.convert('RGB')\n",
    "transposed  = im.transpose(Image.ROTATE_270)\n",
    "\n",
    "\n",
    "transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_sample_arr = gray_sample_arr.transpose() #Reformat \n",
    "min_arr = gray_sample_arr.min()\n",
    "max_arr = gray_sample_arr.max()\n",
    "gray_sample_arr = (gray_sample_arr - min_arr) / (max_arr - min_arr) #Normalizes all values to 0 - 1\n",
    "gray_sample_arr = gray_sample_arr * 255  #Multiply everything to RGB Scale\n",
    "im = Image.fromarray(gray_sample_arr)\n",
    "im = im.convert('RGB')\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_sample_arr = np.zeros((1058, 400))\n",
    "with open(\"C:/Users/19174/Desktop/BP HPC/small_dataset/small_dataset/img_1.stack.segy\", 'rb') as segy_in_file:\n",
    "    segy_reader = create_reader(segy_in_file, endian='>')\n",
    "    for trace_index in segy_reader.trace_indexes():\n",
    "        data = segy_reader.trace_samples(trace_index)\n",
    "        for i in range(len(data)):\n",
    "            gray_sample_arr[trace_index, i] = data[i]\n",
    "gray_sample_arr = gray_sample_arr.transpose() #Reformat \n",
    "min_arr = gray_sample_arr.min()\n",
    "max_arr = gray_sample_arr.max()\n",
    "gray_sample_arr = (gray_sample_arr - min_arr) / (max_arr - min_arr) #Normalizes all values to 0 - 1\n",
    "gray_sample_arr = gray_sample_arr * 255  #Multiply everything to RGB Scale\n",
    "im = Image.fromarray(gray_sample_arr)\n",
    "im = im.convert('RGB')\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSIM Across Tiles \n",
    "Calculate SSIM for tiles across the realizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mean_ssim_quick.pickle', 'rb') as f: #Approach: SSIM across 2 matrices, multichannel = False\n",
    "    ans = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ssim_map.pickle', 'rb') as f: #Approach: Convert matrix to image, multichannel = True\n",
    "     ssim_map = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REALS = 39\n",
    "map_reals = {}\n",
    "for _ in range(1, REALS + 1):\n",
    "    path = \"C:/Users/19174/Desktop/BP HPC/small_dataset/small_dataset/img_\" + str(_) +  \".stack.segy\"\n",
    "    arr = np.zeros((1058, 400))\n",
    "    with open(path, 'rb') as segy_in_file:\n",
    "        segy_reader = create_reader(segy_in_file, endian='>')\n",
    "        for trace_index in segy_reader.trace_indexes():\n",
    "            data = segy_reader.trace_samples(trace_index)\n",
    "            for i in range(len(data)):\n",
    "                arr[trace_index, i] = data[i] #Collecting data for sample_arr\n",
    "    map_reals[_] = arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non Overlapping Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Template of breaking up a single realization into 23X20 tiles\n",
    "#Don't actually need to run\n",
    "hori_size = 46\n",
    "vert_size = 20\n",
    "ans = []\n",
    "for i in range(int(1058 / hori_size)):\n",
    "    test = []\n",
    "    for j in range(int(400 /vert_size)):\n",
    "        test.append([])\n",
    "    ans.append(test)\n",
    "REALS = 39\n",
    "for _ in range(1, REALS + 1):\n",
    "    for temp in range(_ + 1, REALS + 1):\n",
    "        first_data = map_reals[_]\n",
    "        second_data = map_reals[temp]\n",
    "        print(_, temp)\n",
    "        x_index = 0\n",
    "        y_index = 0\n",
    "        for i in range(0, 1058, hori_size):\n",
    "            for j in range(0, 400 , vert_size):\n",
    "                first_map = np.zeros((hori_size,vert_size))\n",
    "                second_map = np.zeros((hori_size,vert_size))\n",
    "                #k and m responsible for partition array\n",
    "                for k in range(i, i + hori_size):\n",
    "                    data1 = first_data[k]\n",
    "                    data2 = second_data[k]\n",
    "                    for m in range(j, j + vert_size): #Loop sets matrix to these elements\n",
    "                        x = k% hori_size\n",
    "                        y = m % vert_size\n",
    "                        first_map[x, y] = data1[m]\n",
    "                        second_map[x, y] = data2[m]\n",
    "                s = ssim(first_map, second_map, multichannel= False)\n",
    "                ans[x_index][y_index].append(s)\n",
    "                y_index += 1\n",
    "            x_index += 1\n",
    "            y_index = 0\n",
    "for i in range(len(ssim_map)):\n",
    "    for j in range(len(ssim_map[0])):\n",
    "        ans[i][j] = np.mean(ans[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlapping Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to optimize overlapping sliding window\n",
    "#Right now, it repeats over visited elements\n",
    "REALS = 39\n",
    "hori_size = 46\n",
    "vert_size = 20\n",
    "mean_pixel = {}\n",
    "for i in range(1058):\n",
    "    for j in range(400):\n",
    "        mean_pixel[(i, j)] = []\n",
    "def helper(arr, x, y):\n",
    "    ans = []\n",
    "    for i in range(x, x + hori_size):\n",
    "        temp = []\n",
    "        for j in range(y, y + vert_size):\n",
    "            temp.append(arr[i][j])\n",
    "        ans.append(temp)\n",
    "    return ans\n",
    "for _ in range(1, REALS + 1):\n",
    "    for temp in range(_ + 1, REALS + 1):\n",
    "        first_data = map_reals[_] #Grabs all data from first file\n",
    "        second_data = map_reals[temp] #Grabs all data from second file\n",
    "        print(\"Files Compared: \", _, temp)\n",
    "        for i in range(0, 1058 - hori_size, 1):\n",
    "            first_map = helper(first_data, 0, i)\n",
    "            second_map = helper(second_data, 0, i)\n",
    "            s = ssim(np.array(first_map), np.array(second_map))\n",
    "            for a in range(hori_size):\n",
    "                for b in range(i, i + vert_size):\n",
    "                    mean_pixel[(a, b)].append(s)\n",
    "            for j in range(vert_size - 1 , 400 - vert_size, 1):\n",
    "                first_map.pop(0)\n",
    "                dummy = list(first_data[j][i : i + vert_size])\n",
    "                first_map.append(dummy)\n",
    "                second_map.pop(0)\n",
    "                dummy = list(second_data[j][i : i + vert_size])\n",
    "                second_map.append(dummy)\n",
    "                s = ssim(np.array(first_map), np.array(second_map)) #shape wrong\n",
    "                for a in range(i + hori_size):\n",
    "                    for b in range(j + vert_size):\n",
    "                        mean_pixel[(a, b)].append(s) \n",
    "            print(i + 1, \"/1058 Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to optimize overlapping sliding window\n",
    "#Right now, it repeats over visited elements\n",
    "REALS = 39\n",
    "hori_size = 46\n",
    "vert_size = 20\n",
    "mean_pixel = {}\n",
    "for i in range(1058):\n",
    "    for j in range(400):\n",
    "        mean_pixel[(i, j)] = []\n",
    "def helper(arr, x, y):\n",
    "    \"\"\"\n",
    "    Given array and top left indices of partition array, return the tile\n",
    "    \"\"\"\n",
    "    ans = []\n",
    "    for i in range(x, x + hori_size):\n",
    "        temp = []\n",
    "        for j in range(y, y + vert_size):\n",
    "            temp.append(arr[i][j])\n",
    "        ans.append(temp)\n",
    "    return ans\n",
    "for _ in range(1, REALS + 1):\n",
    "    for temp in range(_ + 1, REALS + 1):\n",
    "        first_data = map_reals[_] #Grabs all data from first file\n",
    "        second_data = map_reals[temp] #Grabs all data from second file\n",
    "        print(\"Files Compared: \", _, temp)\n",
    "        for i in range(0, 1058 - hori_size, 1):\n",
    "            for j in range(0, 400 - vert_size, 1):\n",
    "                first_map = helper(first_data, i, j)\n",
    "                second_map = helper(second_data, i, j)\n",
    "                s = ssim(np.array(first_map), np.array(second_map))\n",
    "                for a in range(i + hori_size):\n",
    "                    for b in range(j + vert_size):\n",
    "                        mean_pixel[(a, b)].append(s) \n",
    "            print(i + 1, \"/1058 Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ssim_map)):\n",
    "    for j in range(len(ssim_map[0])):\n",
    "        ans[i][j] = np.mean(ans[i][j])\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ssim_map.pickle', 'wb') as f:\n",
    "    pickle.dump(ans, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mean_ssim_quick.pickle', 'wb') as f:\n",
    "    pickle.dump(ans, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ssim_map)):\n",
    "    for j in range(len(ssim_map[0])):\n",
    "        ssim_map[i][j] = np.mean(ssim_map[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_REAL = \"C:/Users/19174/Desktop/BP HPC/large_dataset_only_realizations/img_5.stack.segy\"\n",
    "sample_arr = np.zeros([1058, 400, 3], dtype=np.uint8)\n",
    "gray_sample_arr = np.zeros((1058, 400))\n",
    "with open(SAMPLE_REAL, 'rb') as segy_in_file:\n",
    "    segy_reader = create_reader(segy_in_file, endian='>')\n",
    "    for trace_index in segy_reader.trace_indexes():\n",
    "        data = segy_reader.trace_samples(trace_index)\n",
    "        for i in range(len(data)):\n",
    "            gray_sample_arr[trace_index, i] = data[i]\n",
    "min_arr = gray_sample_arr.min()\n",
    "max_arr = gray_sample_arr.max()\n",
    "gray_sample_arr = (gray_sample_arr - min_arr) / (max_arr - min_arr) #Normalizes all values to 0 - 1\n",
    "gray_sample_arr = gray_sample_arr * 255\n",
    "for i in range(len(gray_sample_arr)):\n",
    "    for j in range(len(gray_sample_arr[0])):\n",
    "        sample_arr[i][j] = [gray_sample_arr[i][j], gray_sample_arr[i][j], gray_sample_arr[i][j]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(sample_arr)\n",
    "im = im.convert('RGB')\n",
    "transposed  = im.transpose(Image.ROTATE_270)\n",
    "transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = .7\n",
    "min_val = float('inf')\n",
    "max_val = -float('inf')\n",
    "for subarray in ans:\n",
    "    for element in subarray:\n",
    "        min_val = min(min_val, element)\n",
    "        if element <= THRESHOLD:\n",
    "            max_val = max(max_val, element)\n",
    "ans_image = np.zeros([1058, 400, 4], dtype=np.uint8)\n",
    "for i in range(23):\n",
    "    for j in range(20):\n",
    "        if ans[i][j] <= THRESHOLD:\n",
    "            color_intensity = (ans[i][j] - min_val) / (max_val - min_val) * 255\n",
    "            for k in range(i * 46, i*46 + 46):\n",
    "                for m in range(j * 20, j* 20 + 20):\n",
    "                    ans_image[k][m] = [color_intensity, 0, 0, 255] \n",
    "        else:\n",
    "            for k in range(i * 46, i*46 + 46):\n",
    "                for m in range(j * 20, j* 20 + 20):\n",
    "                    ans_image[k][m] = [0, 0, 0, 0] #Make pixel transparent\n",
    "\n",
    "im2 = Image.fromarray(ans_image)\n",
    "im2 = im2.convert('RGBA')\n",
    "transposed2 = im2.transpose(Image.ROTATE_270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(transposed, cmap='gray') # I would add interpolation='none'\n",
    "plt.title(\"SSIM Partitions over Sample Realization Less than .7\")\n",
    "plt.imshow(transposed2, cmap='jet', alpha=.3) # interpolation='none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(len(ans)):\n",
    "    for j in range(len(ans[0])):\n",
    "        test.append(ans[i][j])\n",
    "df = pd.Series(test)\n",
    "df.plot(kind='hist', figsize = (25, 6), bins=200, title = \"Distribution of SSIM Values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = .7\n",
    "min_val = float('inf')\n",
    "max_val = -float('inf')\n",
    "for subarray in ssim_map:\n",
    "    for element in subarray:\n",
    "        min_val = min(min_val, element)\n",
    "        if element <= THRESHOLD:\n",
    "            max_val = max(max_val, element)\n",
    "ssim_image = np.zeros([1058, 400, 4], dtype=np.uint8)\n",
    "for i in range(23):\n",
    "    for j in range(20):\n",
    "        if ssim_map[i][j] <= THRESHOLD:\n",
    "            color_intensity = (ssim_map[i][j] - min_val) / (max_val - min_val) * 255\n",
    "            for k in range(i * 46, i*46 + 46):\n",
    "                for m in range(j * 20, j* 20 + 20):\n",
    "                    ssim_image[k][m] = [color_intensity, 0, 0, 255] \n",
    "        else:\n",
    "            for k in range(i * 46, i*46 + 46):\n",
    "                for m in range(j * 20, j* 20 + 20):\n",
    "                    ssim_image[k][m] = [0, 0, 0, 0] #Make pixel transparent\n",
    "\n",
    "im2 = Image.fromarray(ssim_image)\n",
    "im2 = im2.convert('RGBA')\n",
    "transposed2 = im2.transpose(Image.ROTATE_270)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(transposed, cmap='gray') # I would add interpolation='none'\n",
    "plt.title(\"SSIM Partitions over Sample Realization Less than .7\")\n",
    "plt.imshow(transposed2, cmap='jet', alpha=.3) # interpolation='none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(len(ssim_map)):\n",
    "    for j in range(len(ssim_map[0])):\n",
    "        test.append(ssim_map[i][j])\n",
    "df = pd.Series(test)\n",
    "df.plot(kind='hist', figsize = (25, 6), bins=200, title = \"Distribution of SSIM Values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The provided files ending with a .gather.segy suffix store offset pair gathers generated\n",
    "using some plausible (but not perfect) velocity model. Each offset pair gather file corresponds\n",
    "with a single realization.\n",
    "\n",
    "These gather files are 3D, with 39 offset pairs in the survey. The z and x dimensions are the\n",
    "same as the realizations, with an added dimension indicating the offset pair. This information\n",
    "can be seen in the SEGY header displayed below.\n",
    "\"\"\"\n",
    "print_segy_info('Desktop/BP HPC/small_dataset/small_dataset/img_3.gather.segy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load gathers into a 3D numpy array with dimensions (x, offsets, z).\n",
    "\"\"\"\n",
    "arr = np.zeros((1058, 39, 400))\n",
    "with open('Desktop/BP HPC/small_dataset/small_dataset/img_3.gather.segy', 'rb') as segy_in_file:\n",
    "    segy_reader = create_reader(segy_in_file, endian='>')\n",
    "    \n",
    "    count = 0\n",
    "    for trace_index in segy_reader.trace_indexes():\n",
    "        data = segy_reader.trace_samples(trace_index)\n",
    "        for i in range(len(data)):\n",
    "            arr[int(trace_index / 39), trace_index % 39, i] = data[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.min(), arr.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Render the loaded gathers.\n",
    "\"\"\"\n",
    "\n",
    "nrows = 1\n",
    "ncols = 10\n",
    "f, axarr = matplotlib.pyplot.subplots(nrows, ncols, figsize = (30, 25))\n",
    "\n",
    "for c in range(ncols):\n",
    "    # Pull out all offset pairs for a given x\n",
    "    sub_arr = arr[c, :, :]\n",
    "    sub_arr = sub_arr.transpose()\n",
    "    min_arr = sub_arr.min()\n",
    "    max_arr = sub_arr.max()\n",
    "    sub_arr = (sub_arr - min_arr) / (max_arr - min_arr)\n",
    "    sub_arr = sub_arr * 255\n",
    "    \n",
    "    axarr[c].imshow(sub_arr, cmap = 'gray')\n",
    "    \n",
    "    title = str(c)\n",
    "    axarr[c].set_title(title.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
